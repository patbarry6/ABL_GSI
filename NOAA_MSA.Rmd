---
title: 'NOAA Chinook and Chum salmon mixed stock analysis'
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Patrick Dylan Barry"
date: "15 April 2019"
output:
  html_document:
    df_print: paged
bibliography: ./Bibliography/NOAA_MSA.bib
---

```{r setup, include = FALSE}
# !diagnostics off
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE
)
```

```{r loadpackages, include = FALSE}
library("stringr")
library("tidyverse")
library("rubias")
```

#Introduction
This project was stated to help with the NOAA Chinook and chum salmon Mixed Stock Analysis (MSA). At the NPRMC meeting on 14 April 2019, Jordan Watson and Chuck Guthrie mentioned they were transitioning from Bayes to rubias for MSA estimates. I have been working with both and have a few scripts that I thought might be pretty helpful to them. This notebook details the scripts that should help and troubleshoots any kind of issues that may be encountered when applying those scripts to other files. 

# Chinook Salmon

## Description of the Baseline file {#baseline}
The big differnece between the Chinook and the Chum baseline and mixutre files is that the Chinook baseline uses SNP markers and the Chum salmon baseline uses microsatellite markers. It doesn't make much difference other than the fact that our scripts need to have the option to use either 2 or 3 digit genotypes in Genepop.

So I haven't played with the NOAA Chinook or chum salmon baseline or mixture files before so let's take a quick look at what they look like. What how many and what loci are we dealing with?

```{r ChinookBaseline}
dat<-readLines("./Data/Chinook/ChinookBaseline.gen")
PopIndex<-grep("pop|Pop|POP",dat) %>%
  {if(1%in%.) .[-1]} #Remove the first index if Pop is in the file description
nloci<-PopIndex[1]-2
LociNames<-dat[2:(nloci+1)]
LociNames

Alleles<-dat[-PopIndex]%>%
  .[-(1:(nloci+1))]%>%
  str_split(.,",")%>%
  lapply(.,`[[`,2)%>%
  {str_trim(unlist(.))}

Alleles[1]
```
So there are `r nloci` single nucleotide polymorphism (SNPs) in the Chinook salmon baseline. And all the SNPs are coded as two digit alleles. We know that there are `r length(PopIndex) ` *POP* designators in the baseline file, so we should have that many populations. What do the individual identifiers look like?

```{r IndIdentify}
Inds<-dat[-PopIndex]%>%
  .[-(1:(nloci+1))]%>%
  str_split(.,",")%>%
  lapply(.,`[[`,1)%>%
  {str_trim(unlist(.))}
head(Inds,5)
```
It appears that each individual identifier is a population and individual concatenated with a period. It could be however that the it is the population, year of collection and individual identifier. What does the L mean?

```{r PopNames}
Pops<-dat[-PopIndex]%>%
  .[-(1:(nloci+1))]%>%
  str_split(.,",")%>%
  lapply(.,`[[`,1)%>%
  {str_trim(unlist(.))}%>%
  str_split(.,"\\.")%>%
  lapply(.,`[[`,1)%>%
  unique()%>%
  unlist()

head(Pops,5)
```

In the genepop file we have `r length(PopIndex)` *Pop* designators and `r length(Pops)` unique descriptors based on the individual identifiers. It doesn't appear that the number is something we have to worry about.

In summary, we have `r length(Inds)` individuals from `r length(PopIndex)` populations genotyped at  `r nloci` biallelic SNP loci. 

When converting the mixture file I found out that one of our markers is actually haploid! Locus #6 (C3N3) should be scored as haploid. Genepop accomodates both diploid and haploid data, so we need to do a little bit of editing of the baseline file that I was sent. 

```{r HaploidEdit}
digits=2
dat<-readLines("./Data/Chinook/ChinookBaseline.gen")
  
PopIndex<-grep("pop|Pop|POP",dat) %>%
  {if(1%in%.) .[-1]} #Remove the first index if Pop is in the file description
nloci<-PopIndex[1]-2 # How many loci do we have?
LociNames<-dat[2:(nloci+1)] #What are the loci names?


npops<-length(PopIndex) #How many pops?
dat<-dat[-c(1:(nloci+1),PopIndex)] #Lets get just the geno data
dat2<-matrix(unlist(str_split(dat,",")),nrow=length(dat),ncol=2,byrow=T) #make the geno data a matrix
nInd<-nrow(dat2)

#What are the names of the pops
pops<-dat%>%
  str_split(.,",")%>%
  lapply(.,`[[`,1)%>%
  {str_trim(unlist(.))}%>%
  str_split(.,"\\.")%>%
  lapply(.,`[[`,1)%>%
  unique()%>%
  unlist()

#Pull the pop designation for each individual. Should match rep(pops,each=popCounts)
Indpops<-dat%>%
  str_split(.,",")%>%
  lapply(.,`[[`,1)%>%
  {str_trim(unlist(.))}%>%
  str_split(.,"\\.")%>%
  lapply(.,`[[`,1)%>%
  unlist()

popCountIndex<-lapply(1:npops,function(x) which(Indpops==pops[x])) #make an index for each population of individuals

Genos<-str_split(dat2[,2],pattern = " ") #split the character vector of genotype for each individual
Genos<-lapply(1:nrow(dat2), function(x) Genos[[x]][Genos[[x]]!=""]) 
GenoMat<-matrix(data=unlist(Genos),nrow=nrow(dat2),ncol=nloci,byrow=T)
HaploidLoci<-GenoMat[,6]
GenoMat<-GenoMat[,-6]

HaploidGeno<-lapply(1:length(HaploidLoci), function(x) substring(HaploidLoci[x],seq(1,digits+1,digits),seq(digits,digits*2,digits))) #pull the haploid locus
all(unlist(lapply(1:length(HaploidGeno),function(x) HaploidGeno[[x]][1]==HaploidGeno[[x]][2]))) #check to make sure that they are all coded correctly
HaploGeno<-lapply(HaploidGeno,`[[`,1) #pull the first allele as the haplotype
GenoMat<-cbind(GenoMat,HaploGeno) #move it to the end of the file
LociNames<-c(LociNames[-6],LociNames[6]) #fix the loci names!

Genos<-sapply(1:nrow(GenoMat),function(x) paste(dat2[x,1],",",paste(GenoMat[x,],collapse=" "),sep=" ")) #paste them all together in genepop format

#write out each population in a loop
NewFileName<-"./Data/Chinook/ChinookBaseline_clean.gen"
write.table(file=NewFileName,x="Cleaned up BaselineFile",col.names=F,row.names=F,quote=F)
    write.table(file=NewFileName,x=LociNames,append=T,col.names = F,row.names=F,quote=F)
    for (p in 1:npops){
      write.table(file=NewFileName,x="Pop",append=T,col.names = F,row.names=F,quote=F)
      write.table(file=NewFileName,x=Genos[eval(parse(text=popCountIndex[p]))],append=T,col.names = F,row.names=F,quote=F)
    }

```
Cool so it looks like we have taken the mtDNA locus, coded it correctly and moved it to the end of the file. 

We have a choice now. We can quickly recode the mixture file we were given so that we can work on the Genepop2rubias functions, or we can skip ahead and use Chucks input files and work on the Bayes2Rubias functions. 


### Chum Baseline file
Jordan Watson suggested that the chum baseline file is really just an allele frequency file. So we will need to simulate a bunch of genotypes for it to work with *rubias*.  

##Description of the Mixture file
Each year the mixture file will be created from either the microsatellite or SNP data. So we should get a file like the one that is error checked after genotying. 

### Chinook \& Chum salmon
I assume it will be something like Mixure*Year*.1. This would mean that we don't really need to much work to the *Genepop2rubias_baseline()* function to make it work with a baseline file.

## Conversion script 
### *Genepop2rubias*
So for my PhD I had a few Sockeye salmon that were huge outliers and I wanted to see if I could assign them to nearby populations using the ADF&G sockeye baseline. Luckily, I had used the SNPs for my study that they use in their baseline. So I had on hand some scripts that could easily be turned into functions for use by the NOAA folks. I made two functions *Genepop2rubias_baseline()* and *Genepop2rubias_mixture()*.
From the [baseline file](#chinookbaseline) file I was able to adapt my script to read in a genepop file and spit out a .csv for use in rubias. Here is what the funciton needs as input:

*Genepop2rubias_baseline()*
  
  + infile *character* Name of genepop input file. If it is located in a different folder give the entire       file path.  
  + outfile *character* File path of the output file. If you want it to save somethwere else give it the        entire file path.
  + ReportingGroupFile *character* File path to the reporting group file. This should be a csv with the         first column as the           population label in the genepop file and the second column as the reporting group       used for the analysis
  + digits *numeric* The number of digits used for the genepop file (can be 2 or 3).

*Genepop2rubias_mixture()*

  + infile *character* Name of genepop input file. If it is located in a different folder give the entire file path.  
  + outfile *character* File path of the output file. If you want it to save somethwere else give it the entire file path.
  + digits *numeric* The number of digits used for the genepop file (can be 2 or 3).

It does take a hot second to convert a big file because it splits the concatenated genotypes and places them in separate columns and it loops over the columns. The more loci that are involved the more loops that need to occur. Fun stuff. We could make it faster with a foreach %dopar% loop, but we are likely not going to be using this script a ton. The ShinyApp should be using the formatted rubias files. 

```{r Genepop2rubias, eval=FALSE}
source("./functions/Genepop2rubias.R")
dir.create("./Data/Chinook/rubias_Akutan")
Genepop2rubias_mixture(infile="./Data/Chinook/17AkutanMix.gen",outfile="./Data/Chinook/rubias_Akutan/17AkutanMix.csv",digits=2)

Genepop2rubias_baseline(infile="./Data/Chinook/ChinookBaseline_clean.gen",outfile="./Data/Chinook/rubias_Akutan/ChinookBaselineRubias.csv",digits=2,ReportingGroupFile="./Data/Chinook/ReportingGroups.csv")
```

### *Bayes2Genepop*
The *Bayes* baseline file consists of allele counts at each locus for all the populations. So we don't have raw multilocus genotypes for each individual. We could take the *Bayes* baseline and simulate those genotypes, but that seems silly when we have the Genepop file to work with. We may have to simulate a Genepop baseline file for chum salmon but we will cross that bridge when we come to it. 

```{r Bayes2rubias, eval=FALSE}
source("./functions/Bayes2Genepop.R")
Bayes2Genepop_mixture(infile="./Data/Chinook/2016_Akutan.MIX",outfile="./Data/Chinook/2016_Aukutan.gen",LociFile="./Data/Chinook/Loci.txt",digits=2,SampPrefix="Mix",Project=paste(infile,"Bayes2Genepop",sep=""))
```
So to create the Genepop2rubias scripts I needed Genepop files. Chuck gave me two Bayes files, so I first had to create a genepop file from the Bayes file which made the mixture conversion function a snap to create. Why convert it to Genepop first? Well it might be useful to run it through ade4 and a individual based PCA to see how different the samples are based on their multilocus genotype. Also, genepop is the most widely used input file and most of my code starts with a Genepop file, so it is easy to pipe it into another script to convert to rubias. 

The only snafu I ran into was that the original mixture file had only a single allele called for some individuals at locus 6. This actually was really good because I didn't know there was a haploid marker in the panel. 

Here is what the funciton needs as input:

  + infile *character* Name of mixture file. If it is located in a different folder give the entire file path.  
  + outfile *character* File path of the output file. If you want it to save somethwere else give it the entire file path.
  + LociFile *character* File path to a file with all the locus names. This should be a plain text file with loci on individual lines.
  + digits *numeric* The number of digits used for the genepop file (can be 2 or 3).
  + project *character* This is the project name that will go at the top of your genepop file. It defaults to the name of the input file and Bayes2Genepop
  
  
##Akutan Analysis: *Bayes* vs. *Rubias*

###*Bayes*
Chuck gave me a control file for Bayes to run the Akutan mixture. I assume this is the baseline file used in Guthrie et al. [-@Guthrie2018]. To run multiple chains I need to create a few more control files. I could ask Chuck for them, but it should be pretty straighforward to change the names of the output files, change the random seeds, change the starting proportions for the baseline samples and produce say 3 more control files:

```{r BayesCtrlFiles, eval=FALSE}
dir.create("./Data/Chinook/Bayes_Akutan")
file.copy(from="./Data/Chinook/17Akutan-1.csv",to="./Data/Chinook/Bayes_Akutan/17Akutan.ctrl")
Samps<-500000 #Chuck was running 10000 in his Ctrl file, he likely runs a bunch of short chains?

for (ctr in 1:4){
Ctrl<-readLines("./Data/Chinook/Bayes_Akutan/17Akutan.ctrl")
Title<-str_split(Ctrl[1]," ")[[1]][1]
Ctrl[4]<-paste(Title,"-",ctr,".SUM",sep="")
Ctrl[5]<-paste(Title,"-",ctr,".BOT",sep="")
Ctrl[6]<-paste(Title,"-",ctr,".FRQ",sep="")
Ctrl[7]<-paste(Title,"-",ctr,".B01",sep="")
Ctrl[8]<-paste(Title,"-",ctr,".CLS",sep="")
Ctrl[9]<-paste(Title,"-",ctr,".RGN",sep="")
Ctrl[10]<-format(Samps,scientific = F) #how many MCMC samples
Ctrl[16:17]<-10 #thinning of stock proportion and baseline allele freq
Ctrl[18]<-100 #thinning of stock assignment of each mixture individual
Ctrl[13:15]<-sample(1:2147483647,3,replace=F) #random seeds
#now for the starting stock proportions... this might be tricksy
StockProp<-matrix(data=unlist(str_split(Ctrl[(length(Ctrl)-as.numeric(Ctrl[11])+1):length(Ctrl)],"\\s+")),nrow=as.numeric(Ctrl[11]),ncol=5,byrow=T)

if(ctr==1){
  cat("Leaving the starting proportions alone!")
}
if(ctr==2){
  StockProp[,5]<-gsub("0\\.","\\.",sprintf(1/as.numeric(Ctrl[11]), fmt = '%#.6f'))
}
if(ctr==3){
  StockProp[which(StockProp[,2]==1),5]<-gsub("0\\.","\\.",sprintf(0.5/nrow(StockProp[which(StockProp[,2]==1),]), fmt = '%#.6f'))
  StockProp[-which(StockProp[,2]==1),5]<-gsub("0\\.","\\.",sprintf(0.5/nrow(StockProp[-which(StockProp[,2]==1),]), fmt = '%#.6f'))
}

if(ctr==4){
  StockProp[which(StockProp[,2]==1|StockProp[,2]==2),5]<-gsub("0\\.","\\.",sprintf(0.5/nrow(StockProp[which(StockProp[,2]==1|StockProp[,2]==2),]), fmt = '%#.6f'))
  StockProp[-which(StockProp[,2]==1|StockProp[,2]==2),5]<-gsub("0\\.","\\.",sprintf(0.5/nrow(StockProp[-which(StockProp[,2]==1|StockProp[,2]==2),]), fmt = '%#.6f'))
}

Ctrl[(length(Ctrl)-as.numeric(Ctrl[11])+1):length(Ctrl)]<-paste(formatC(StockProp[,1],width=4,flag="-"),formatC(StockProp[,2],width=3,flag="-"),formatC(StockProp[,3],width=9,flag="-"),formatC(StockProp[,4],width=19,flag="-"),formatC(StockProp[,5],width=7,flag="-"),sep="")

write.table(x=as.vector(Ctrl),file=paste("./Data/Chinook/Bayes_Akutan/",Title,"-",ctr,".ctrl",sep=""),quote=F,row.names = F,col.names = F)
}

```
Now that we have 4 brand new control files for Bayes we can walk to the computer lab and work on a windows machine and run these bad boyz. It might take a while, so lets create 4 directories for *Bayes* each with their own executable and run them in parallel. Now that windows 10 has a linux kernel can we write a shell script and just launch them that way? Likely, but for the sake of getting this done and working on my Ph.D we will just copy and paste the entire directory and launch Bayes four times. After they are done we can check for convergence using the Gelmen-Rubin statistic and then ask for the proportion of each baseline population in the mixture. Then we can compare with what we got from *rubias*.  

```{r BayesAkutanConverge, eval=TRUE}
#check to make sure that the chains converged
Res<-readLines("./Data/Chinook/Bayes_Akutan/17Akutan-1.SUM")
StockNum<-11
Res[(grep("Gelman and Rubin diagnostics computed from the second half of each chain only.",Res)+2):(grep("Gelman and Rubin diagnostics computed from the second half of each chain only.",Res)+3+StockNum)]

```

It appears that our four Bayes chaines with different initial stock compositions converged to a similar solution. That is good. I didn't change the starting proportions that much, but they were substantially different from what we inferred so I will take that as a good sign. Let's now compare the stock composition estimates between the two programs.

```{r BayesAkutanComp, eval=FALSE}
Res<-readLines("./Data/Chinook/Bayes_Akutan/17Akutan-1.SUM")
StockNum<-11
StockProp_Bayes<-Res[(grep("Chains combined: ",Res)+2):(grep("Chains combined: ",Res)+3+StockNum)]

StockRepMap<-read.csv("./Data/Chinook/ReportingGroups.csv")[,-1]%>%
  group_by(ReportingGroup)%>%
  filter(row_number()==1)%>%
  as.data.frame()

StockPropMat<-matrix(data=unlist(stringr::str_split(StockProp_Bayes[-(1:2)],"\\s+")),nrow=StockNum,ncol=9,byrow = T)[,-c(1:2)]

StockPropMat<-StockPropMat[,c(1,5,4,6)]

StockPropMat[,1]<-plyr::mapvalues(as.numeric(StockPropMat[,1]),from=StockRepMap[,2],to=as.character(StockRepMap[,1]))

AkutanMix<-read.csv("./Data/Chinook/rubias_Akutan/17AkutanMix.csv",stringsAsFactors = F)
ChinookBase<-read.csv("./Data/Chinook/rubias_Akutan/ChinookBaselineRubias.csv",stringsAsFactors = F)
mix_est <- infer_mixture(reference = ChinookBase, 
                         mixture = AkutanMix, 
                         gen_start_col = 5,reps = 200000, burn_in = 20000)


RG_mix_ests <- mix_est$mixing_proportions %>%
  group_by(mixture_collection, repunit) %>%
  summarise(repprop = sum(pi)) 

trace_subset <- mix_est$mix_prop_traces %>%
  filter(mixture_collection == "Mix", sweep > 50000) %>%
  group_by(sweep, repunit) %>%
  summarise(repprop = sum(pi)) 

CI_rub <- trace_subset %>%
  group_by(repunit) %>%
  summarise(loCI = quantile(repprop, probs = 0.025),
            hiCI = quantile(repprop, probs = 0.975))

RubiasRes<-cbind(RG_mix_ests[,2:3],CI_rub[,2:3])
RubiasRes$InfMeth<-"Rubias"

BayesRes<-cbind(StockPropMat,rep("Bayes",nrow(StockPropMat)))
colnames(BayesRes)<-colnames(RubiasRes)

Res<-rbind(RubiasRes,BayesRes)
Res[,2]<-as.numeric(Res[,2])
Res[,3]<-as.numeric(Res[,3])
Res[,4]<-as.numeric(Res[,4])

Res[,1]<-factor(Res[,1],levels(as.factor(Res[,1]))[c(9,3,5,10,6,8,4,7,2,1,11)])

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(Res, aes(x=repunit, y=repprop, fill=InfMeth)) + 
  geom_bar(stat="identity", color="black", 
           position=position_dodge()) +
  geom_errorbar(aes(ymin=loCI, ymax=hiCI), width=.2,
                 position=position_dodge(.9)) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  scale_fill_manual(values=cbPalette[2:3])+
  guides(fill=guide_legend(title="Inference \nMethod"))+
  xlab("Reporting Group")+
  ylab("Stock Composition")

```

This looks great! Bayes and Rubias give really similar results. I took the median values for Bayes with the 95% CI values. This, it appears, is what Chuck reports in the NOAA tech memos each year. We can also look at the mean value with standard deviations to see if they differ much:

```{r BayesAkutanCompMean, eval=TRUE}
#check to make sure that the chains converged
StockPropMat<-matrix(data=unlist(stringr::str_split(StockProp_Bayes[-(1:2)],"\\s+")),nrow=StockNum,ncol=9,byrow = T)[,-c(1:2)]

StockPropMat<-cbind(StockPropMat[,c(1,2)],as.numeric(StockPropMat[,2])-as.numeric(StockPropMat[,3]),as.numeric(StockPropMat[,2])+as.numeric(StockPropMat[,3]))

StockPropMat[,1]<-plyr::mapvalues(as.numeric(StockPropMat[,1]),from=StockRepMap[,2],to=as.character(StockRepMap[,1]))

BayesRes<-cbind(StockPropMat,rep("Bayes",nrow(StockPropMat)))
colnames(BayesRes)<-colnames(RubiasRes)

Res<-rbind(RubiasRes,BayesRes)
Res[,2]<-as.numeric(Res[,2])
Res[,3]<-as.numeric(Res[,3])
Res[,4]<-as.numeric(Res[,4])

Res[,1]<-factor(Res[,1],levels(as.factor(Res[,1]))[c(9,3,5,10,6,8,4,7,2,1,11)])

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(Res, aes(x=repunit, y=repprop, fill=InfMeth)) + 
  geom_bar(stat="identity", color="black", 
           position=position_dodge()) +
  geom_errorbar(aes(ymin=loCI, ymax=hiCI), width=.2,
                 position=position_dodge(.9)) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  scale_fill_manual(values=cbPalette[2:3])+
  guides(fill=guide_legend(title="Inference \nMethod"))+
  xlab("Reporting Group")+
  ylab("Stock Composition")

```

The mean and median for *Bayes* do not differ substantially, but using the standard deviation on the mean to represent the uncertainty around the estimate makes it appear we have much more confidence in the estimate. 

Overall, we see that *Bayes* and *rubias* don't differ that much for the Akutan mixture. We could run a few different mixtures and compare them. It would be relatively easy to produce a few rubias mixture files and run them in sequence. Running the *Bayes* files would be a pain, but not terrible. I probably don't need to run either program for as long as I have here. *Bayes* takes overnight to run (not sure how long, but it is substantial) whereas *rubias* takes about 30 minutes to run with 500,000 samples. Can we run multiple chains in *rubias* from different starting configurations and assess convergence? Maybe Chuck can give some insight into how often Bayes runs into convergence issues. 

# Chum Salmon
The chum salmon baseline is formatted for Bayes [@Kondzelaetal2013]. So there is no Genepop file to work with here. The trouble with the Bayes baseline file is that alleles are given as counts for each population, but they aren't indexed by allele size. We would need to know the allele sizes used in the baseline provided we are converting from scores hot off the 3730. It isn't that big a deal if we are just converting old baseline and mixture files because we can just use the index of the allele in the mixture and baseline file. Let's do a quick comparison of Bayes and rubias. We will just code the uSat alleles by their index in the baseline and mixture files. So allele sizes should range from 1-30ish. We will definately want to come back to this and refine the code because it is silly to go from the 3730 alleles to a Bayes input just to run these analyses. I wonder if DFO ever gave ABL their sizes and they were standardized across labs. That is way above my pay grade so let's just go forth and compare.

First we need to create a baseline and mixture file:

```{r ChumBaselineMixture, eval=TRUE}
#how many pops?
pops<-readLines("./Data/Chum/Chum_DFO11_BAYES_PDB.bse")%>%
      {sapply(1:length(.),function(l) unlist(str_split(.[l],"\\s+"))[2])}%>%
      as.numeric()%>%
      max()
#how many loci?
nloci<-readLines("./Data/Chum/Chum_DFO11_BAYES_PDB.bse")%>%
        .[grep(pattern="^\\s+1\\s",x=.)]%>%
        length()
#how many alleles per loci?
AllelesLoci<-readLines("./Data/Chum/Chum_DFO11_BAYES_PDB.bse")%>%
    .[grep(pattern="^\\s+1\\s",x=.)]%>%
    str_trim()%>%
    str_split(pattern="\\s+")%>%
    {lapply(1:length(.),function(x) length(.[[x]])-3)}%>%
    unlist()

# We are going to loop over loci and populations and count up the number of alleles
# that were sampled and then just sample without replacement to create 
# genotypes with a baseline with the correct number of each allele

PopMats<-list()
for (p in 1:pops){
  PopSampNum<-readLines("./Data/Chum/Chum_DFO11_BAYES_PDB.bse")%>% #read in the modified baseline
    .[grep(pattern=paste("^\\s+",p,"\\s",sep=""),x=.)]%>% #subset on the population we want
    str_trim()%>% #trim off the leading space
    {sapply(1:length(.),function(l) unlist(str_split(.[l],"\\s+"))[3])}%>% #figure out how many individuals were sampled
    as.numeric()%>%
    max(.)/2 #how many genotypes to create?
  LociList<-list()
  for (l in 1:nloci){
    Alleles<-readLines("./Data/Chum/Chum_DFO11_BAYES_PDB.bse")%>% 
      .[grep(pattern=paste("^\\s+",p,"\\s",sep=""),x=.)]%>% 
      {str_trim(.[l])}%>%
      str_split("\\s+")%>%
      unlist()%>%
      .[-(1:3)]%>%
      as.numeric()
    
    names(Alleles)<-seq(1,length(Alleles),by=1)
    
    Allele2Samp<-as.numeric(rep(names(Alleles),Alleles)) #these are the alleles that we need to sample
    
    Genos<-matrix(data=sample(Allele2Samp,sum(Alleles),replace = F),nrow=sum(Alleles)/2,ncol=2,byrow=T) #these are our genotypes
    
    # #exit if there are an uneven number of alleles
    # if(sum(Alleles)%%2 != 0){
    #   cat(paste("Isse with ",l," locus in ", p, " pop",sep=""))
    #   break
    # }
    # 
    
    #Each locus may have different number of alleles sampled, so lets pad the locus genotypes with NAs to make sure that we have enough genotypes for the entire set of individuals in the population
    if(PopSampNum-nrow(Genos)==1){
      Genos<-rbind(Genos,c(NA,NA))
    } else if(PopSampNum-nrow(Genos)>1){
      UnSamPad<-PopSampNum-nrow(Genos)
      Genos<-rbind(Genos,matrix(data=NA,ncol=2,nrow=UnSamPad))
    } 
  #randomize the rows to make sure all the NAs aren't at the bottom
  Genos<-Genos[sample(1:nrow(Genos),nrow(Genos),replace=F),]
  LociList[[l]]<-Genos
  
  }#over l loci
  PopMats[[p]]<-cbind(seq(from=1,to=PopSampNum,by=1),rep(p,PopSampNum),do.call(cbind,LociList))
}#over p pops

Baseline<-do.call(rbind,PopMats)
BaselineConverted<-Baseline #lets store it as a new matrix for debugging 

#Now we will need to convert all the loci using the conversions that Jackie Whittle sent me
Convert2uSat<-read.csv("./Data/Chum/ConvertuSats.csv")
for (l in 1:nloci){
  BaselineConverted[,(l*2+1):(l*2+2)]<-plyr::mapvalues(x=BaselineConverted[,(l*2+1):(l*2+2)],from=Convert2uSat[which(Convert2uSat[,1]==l),4],to=Convert2uSat[which(Convert2uSat[,1]==l),3],warn_missing = F)
}

LocusNames<-unique(Convert2uSat$LocusName) #pull out the locus names 

#What are the reporting groups?
BaselineRG<-readLines("./Data/Chum/Age3Area1_Chain1.ctl")[-(1:(21+nloci))]%>%
  str_split("\\s+")%>%
  unlist()%>%
  matrix(data=.,nrow=pops,ncol=5,byrow=T)

#Create a matrix so that we can easily swap out the reporting group number for its acutal name
BaselineRG<-BaselineRG[,-c(3,5)]
colnames(BaselineRG)<-c("PopNum","RG_Num","PopName")

RGnames<-matrix(data=c(seq(1,6,1),c("EAsia","NAsia","WAlaska","UpMidYukon","SW_Alaska","E_GOP_PNW")),nrow=6,ncol=2)

#now lets format this for rubias
BaselineConverted<-as.data.frame(BaselineConverted)
BaselineConverted$sample_type<-"reference"
BaselineConverted$collection<-plyr::mapvalues(x=BaselineConverted[,2],from=BaselineRG[,1],to=BaselineRG[,3])
BaselineConverted$collectionNum<-plyr::mapvalues(x=BaselineConverted[,2],from=BaselineRG[,1],to=BaselineRG[,2])
BaselineConverted$repunit<-plyr::mapvalues(x=BaselineConverted$collectionNum,from=RGnames[,1],to=RGnames[,2])

rubiasBL<-BaselineConverted[,c(25,28,26,1,3:(nloci*2+2))]
colnames(rubiasBL)[4]<-"indiv"
colnames(rubiasBL)[5:(nloci*2+4)]<-rep(as.character(LocusNames),each=2)

dir.create("./Data/Chum/rubias")

rubiasBL[,4]<-paste(rubiasBL[,3],"_",rubiasBL[,4],sep="")
write.csv(file="./Data/Chum/rubias/ChumBaseline_rubias.csv",x=rubiasBL,quote=F,row.names = F)
writeLines(text=as.character(LocusNames),con="./Data/Chum/Loci.txt")
```
It appears that there was a slight issue with the Bayes baseline file. Population 240 was scored at locus 2 for 105 alleles (52.5 individuals). I added an extra allele (the most frequent) so that I could proceed with sampling without replacement. The new file is now called Chum_DFO11_BAYES_PDB.bse

Alternatively we could double the size of the population and sample all the alleles twice. This would give us the same allele frequencies but a larger population. In each program the baseline allele frequencies are likely updated. Does the size of the population play a part in how the allele frequencies are estimated? If so this could not be the best way to create our new baseline. First thing we should do is read through Pella and Masuda [-@PellaMasuda2001] and figure out what *Bayes* does. Then maybe shoot Michelle and email and talk to her about which was is preferred. 

For now lets convert the rubias files that we just created with the *Bayes* alllele indexes so that they match the 3730 allele bins.

```{r ConvertRubias, eval=FALSE}
#now lets deal with the mixture
#This should be easy with the script from the King salmon
source("./functions/Bayes2Genepop.R")
Bayes2Genepop_mixture(infile="./Data/Chum/Age3Area1.MIX",outfile="./Data/Chum/Age3Area1.gen",LociFile="./Data/Chum/Loci.txt",digits=3,SampPrefix="Mix",Project="Age3Area1_Bayes2Genepop")
source("./functions/Genepop2rubias.R")
Genepop2rubias_mixture(infile="./Data/Chum/Age3Area1.gen",outfile="./Data/Chum/rubias/Age3Area1.csv",digits=3)

#Our alleles in this file are the index number from the baseline, we need to convert to uSat allele size
rubiasMix<-read.csv("./Data/Chum/rubias/Age3Area1.csv")
rubiasMix<-as.matrix(rubiasMix)
for (l in 1:nloci){
  rubiasMix[,(l*2+3):(l*2+4)]<-plyr::mapvalues(x=str_trim(rubiasMix[,(l*2+3):(l*2+4)]),from=Convert2uSat[which(Convert2uSat[,1]==l),4],to=Convert2uSat[which(Convert2uSat[,1]==l),3],warn_missing = F)
}

write.csv(x=rubiasMix,"./Data/Chum/rubias/Age3Area1.csv",row.names = F)

```
Nice, this worked better than I expected. We have a sweet baseline in the 3730 allele sizes and a mixture that uses those allele sizes as well. It should be noted that if we convert the old mixture files to rubias we will always have to map the values from the file that Jackie gave me to make them compatible. This shouldn't be an issue because they already run Michelle Masudas script to convert from the 3730 scores to a Bayes mixutre file. Maybe I should turn this into a funciton that could easily be sourced. 


```{r ChumBayesCtrlFiles, eval=FALSE}
#Now we need to run bayes and compare. Again we just have a single control file... so lets make at least 4
Samps<-200000 #Let's not go overkill this time
dir.create("./Data/Chum/Bayes")
for (ctr in 1:4){
  Ctrl<-readLines("./Data/Chum/Age3Area1_Chain1.ctl")
  Title<-gsub(" ","",str_split(Ctrl[1],",")[[1]][1])
  Ctrl[2]<-"DFO11_BAYES_PDB.bse"
  Ctrl[4]<-paste(Title,"-",ctr,".SUM",sep="")
  Ctrl[5]<-paste(Title,"-",ctr,".BOT",sep="")
  Ctrl[6]<-paste(Title,"-",ctr,".FRQ",sep="")
  Ctrl[7]<-paste(Title,"-",ctr,".B01",sep="")
  Ctrl[8]<-paste(Title,"-",ctr,".CLS",sep="")
  Ctrl[9]<-paste(Title,"-",ctr,".RGN",sep="")
  Ctrl[10]<-format(Samps,scientific = F) #how many MCMC samples
  Ctrl[16:17]<-10 #thinning of stock proportion and baseline allele freq
  Ctrl[18]<-100 #thinning of stock assignment of each mixture individual
  Ctrl[13:15]<-sample(1:2147483647,3,replace=F) #random seeds
  #now for the starting stock proportions... this might be tricksy
  StockProp<-matrix(data=unlist(str_split(Ctrl[(length(Ctrl)-as.numeric(Ctrl[11])+1):length(Ctrl)],"\\s+")),nrow=as.numeric(Ctrl[11]),ncol=5,byrow=T)
  
  if(ctr==1){
    cat("Leaving the starting proportions alone!")
  }
  if(ctr==2){
    StockProp[,5]<-gsub("0\\.","\\.",sprintf(1/as.numeric(Ctrl[11]), fmt = '%#.6f'))
  }
  if(ctr==3){
    StockProp[which(StockProp[,2]==1),5]<-gsub("0\\.","\\.",sprintf(0.5/nrow(StockProp[which(StockProp[,2]==1),]), fmt = '%#.6f'))
    StockProp[-which(StockProp[,2]==1),5]<-gsub("0\\.","\\.",sprintf(0.5/nrow(StockProp[-which(StockProp[,2]==1),]), fmt = '%#.6f'))
  }
  
  if(ctr==4){
    StockProp[which(StockProp[,2]==1|StockProp[,2]==2),5]<-gsub("0\\.","\\.",sprintf(0.5/nrow(StockProp[which(StockProp[,2]==1|StockProp[,2]==2),]), fmt = '%#.6f'))
    StockProp[-which(StockProp[,2]==1|StockProp[,2]==2),5]<-gsub("0\\.","\\.",sprintf(0.5/nrow(StockProp[-which(StockProp[,2]==1|StockProp[,2]==2),]), fmt = '%#.6f'))
  }
  
  Ctrl[(length(Ctrl)-as.numeric(Ctrl[11])+1):length(Ctrl)]<-paste(formatC(StockProp[,1],width=4,flag="-"),formatC(StockProp[,2],width=3,flag="-"),formatC(StockProp[,3],width=9,flag="-"),formatC(StockProp[,4],width=19,flag="-"),formatC(StockProp[,5],width=7,flag="-"),sep="")
  
  write.table(x=as.vector(Ctrl),file=paste("./Data/Chum/Bayes/",Title,"-",ctr,".ctrl",sep=""),quote=F,row.names = F,col.names = F)
}
```

Not too bad, glad I made that script for the chinook instead of just going in and manually creating the control files. 

```{r ChumRubiasBayes, eval=TRUE, cache.lazy=FALSE}
library(tidyverse)
#Run rubias from within R
Age3Area1<-read.csv("./Data/Chum/rubias/Age3Area1.csv",stringsAsFactors = F)
ChumBase<-read.csv("./Data/Chum/rubias/ChumBaseline_rubias.csv",stringsAsFactors = F)

mix_est <- infer_mixture(reference = ChumBase, 
                         mixture = Age3Area1, 
                         gen_start_col = 5,reps = 200000, burn_in = 50000)

#clean up the results
RG_mix_ests <- mix_est$mixing_proportions %>%
  dplyr::group_by(mixture_collection, repunit) %>%
  dplyr::summarise(repprop = sum(pi)) 

trace_subset <- mix_est$mix_prop_traces %>%
  dplyr::filter(mixture_collection == "Mix", sweep > 50000) %>%
  dplyr::group_by(sweep, repunit) %>%
  dplyr::summarise(repprop = sum(pi)) 

CI_rub <- trace_subset %>%
  dplyr::group_by(repunit) %>%
  dplyr::summarise(loCI = quantile(repprop, probs = 0.025),
            hiCI = quantile(repprop, probs = 0.975))

RubiasRes<-cbind(RG_mix_ests[,2:3],CI_rub[,2:3])
RubiasRes$InfMeth<-"Rubias"


#Pull in the Bayes results
Res<-readLines("./Data/Chum/Bayes/2017-4.SUM")
StockNum<-6
StockProp_Bayes<-Res[(grep("Chains combined: ",Res)+2):(grep("Chains combined: ",Res)+3+StockNum)]

#create a matrix of the results
StockPropMat<-matrix(data=unlist(stringr::str_split(StockProp_Bayes[-(1:2)],"\\s+")),nrow=StockNum,ncol=9,byrow = T)[,-c(1:2)]

#tidy up the results
StockPropMat<-StockPropMat[,c(1,5,4,6)]

#map the reporting group names instead of the numbers
StockPropMat[,1]<-plyr::mapvalues(as.numeric(StockPropMat[,1]),from=RGnames[,1],to=as.character(RGnames[,2]))

#create a matrix that we can bind with the rubias results for graphing
BayesRes<-cbind(StockPropMat,rep("Bayes",nrow(StockPropMat)))
colnames(BayesRes)<-colnames(RubiasRes)

#bind up all the results
Res<-rbind(RubiasRes,BayesRes)
Res[,2]<-as.numeric(Res[,2])
Res[,3]<-as.numeric(Res[,3])
Res[,4]<-as.numeric(Res[,4])

#rearrange the factor level so that the regions are in the order from the report
Res[,1]<-factor(Res[,1],levels(as.factor(Res[,1]))[c(2,3,6,5,4,1)])

#Lets graaph in the color blind pallete
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(Res, aes(x=repunit, y=repprop, fill=InfMeth)) + 
  geom_bar(stat="identity", color="black", 
           position=position_dodge()) +
  geom_errorbar(aes(ymin=loCI, ymax=hiCI), width=.2,
                position=position_dodge(.9)) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  scale_fill_manual(values=cbPalette[2:3])+
  guides(fill=guide_legend(title="Inference \nMethod"))+
  xlab("Reporting Group")+
  ylab("Stock Composition")

```

Oh snap, again Bayes and rubias are pretty similar. Let's redo the analysis and bump up the samples for both programs and increase the burn-in to see if that has any effect.  



#References








